"""Database queries for Expert mode."""

import duckdb
import streamlit as st

from config import (
    EXPERT_PARQUET_DATASET,
    EXPERT_TOP_SPECIES_COUNT,
    S3_ACCESS_KEY_ID,
    S3_ENDPOINT,
    S3_SECRET_ACCESS_KEY,
)


@st.cache_resource
def get_duckdb_connection():
    """Create and configure DuckDB connection."""
    conn = duckdb.connect()
    conn.execute("INSTALL httpfs;")
    conn.execute("LOAD httpfs;")
    conn.execute("SET s3_region='us-east-1';")
    conn.execute(f"SET s3_access_key_id='{S3_ACCESS_KEY_ID}';")
    conn.execute(f"SET s3_secret_access_key='{S3_SECRET_ACCESS_KEY}';")
    conn.execute(f"SET s3_endpoint='{S3_ENDPOINT}';")
    conn.execute("SET s3_use_ssl=true;")
    conn.execute("SET s3_url_style='path';")
    return conn


def check_user_has_annotations(user_id):
    """Check if user has any assigned annotations."""
    try:
        conn = get_duckdb_connection()
        query = (
            f"SELECT COUNT(*) FROM '{EXPERT_PARQUET_DATASET}' "
            "WHERE CAST(userID AS VARCHAR) = CAST(? AS VARCHAR)"
        )
        result = conn.execute(query, [user_id]).fetchone()
        return result[0] > 0 if result else False
    except Exception:
        return False


@st.cache_data
def get_top_species_for_database():
    """Get top N species by detection count for checklist."""
    conn = get_duckdb_connection()
    query = (
        f'SELECT "scientific name", COUNT(*) as count '
        f"FROM '{EXPERT_PARQUET_DATASET}' "
        f'GROUP BY "scientific name" ORDER BY count DESC '
        f"LIMIT {EXPERT_TOP_SPECIES_COUNT}"
    )
    return [row[0] for row in conn.execute(query).fetchall()]


@st.cache_data(
    ttl=1800, show_spinner="Loading assigned clips..."
)  # Cache for 10 minutes
def get_assigned_clips_for_user(user_id):
    """
    Get all assigned clips for a specific userID.
    Returns list of dicts for efficient processing.
    CACHED to avoid repeated parquet queries.

    Args:
        user_id: The user's ID (can be string or int)

    Returns:
        List of dicts with clip information
    """
    conn = get_duckdb_connection()
    query = f"""
    SELECT
        fullPath as filename,
        deployment_id,
        "start time" as start_time,
        "scientific name" as species_array,
        confidence as confidence_array,
        "max uncertainty" as uncertainty_array,
        userID
    FROM '{EXPERT_PARQUET_DATASET}'
    WHERE CAST(userID AS VARCHAR) = CAST(? AS VARCHAR)
    ORDER BY filename, start_time
    """
    results = conn.execute(query, [user_id]).fetchall()

    clips = [
        {
            "filename": row[0],
            "deployment_id": row[1],
            "start_time": row[2],
            "species_array": row[3],
            "confidence_array": row[4],
            "uncertainty_array": row[5],
            "userID": row[6],
        }
        for row in results
    ]

    return clips


@st.cache_data(ttl=300)
def get_validated_pro_clips(user_id):
    """Get clips already validated by this user.
    Returns set of (filename, start_time) tuples.
    """
    import tempfile
    from pathlib import Path

    import boto3
    import pandas as pd
    from botocore.client import Config

    from config import (
        EXPERT_VALIDATIONS_PREFIX,
        S3_ACCESS_KEY_ID,
        S3_BUCKET,
        S3_ENDPOINT,
        S3_SECRET_ACCESS_KEY,
    )

    try:
        s3_client = boto3.client(
            "s3",
            endpoint_url=f"https://{S3_ENDPOINT}",
            aws_access_key_id=S3_ACCESS_KEY_ID,
            aws_secret_access_key=S3_SECRET_ACCESS_KEY,
            config=Config(signature_version="s3v4"),
        )

        response = s3_client.list_objects_v2(
            Bucket=S3_BUCKET, Prefix=EXPERT_VALIDATIONS_PREFIX
        )
        if "Contents" not in response:
            return set()

        all_validations = []
        for obj in response["Contents"]:
            if obj["Key"].endswith(".csv"):
                try:
                    with tempfile.NamedTemporaryFile(
                        mode="w+", suffix=".csv", delete=False
                    ) as temp_file:
                        try:
                            s3_client.download_file(
                                S3_BUCKET, obj["Key"], temp_file.name
                            )
                            df = pd.read_csv(temp_file.name)
                            all_validations.append(df)
                        finally:
                            Path(temp_file.name).unlink()
                except Exception as e:
                    # Skip invalid CSV files during validation loading
                    st.warning(f"Skipping invalid file {obj['Key']}: {e}")
                    continue

        if not all_validations:
            return set()

        combined_df = pd.concat(all_validations, ignore_index=True)
        filtered = combined_df[combined_df["userID"].astype(str) == str(user_id)]
        return set(zip(filtered["filename"], filtered["start_time"], strict=False))

    except Exception:
        return set()


def _get_validated_clips_with_session(user_id):
    """Get all validated clips including session state."""
    validated_clips = get_validated_pro_clips(user_id)
    if hasattr(st.session_state, "expert_validated_clips_session"):
        validated_clips = validated_clips.union(
            st.session_state.expert_validated_clips_session
        )
    return validated_clips


def get_random_assigned_clip(user_id):
    """Get next unvalidated clip for user.
    Uses deterministic ordering (filename, start_time).
    Returns dict with clip info, or 'all_validated': True if done.
    """
    conn = get_duckdb_connection()
    validated_clips = _get_validated_clips_with_session(user_id)

    if validated_clips:
        validated_tuples = [
            (filename, start_time) for filename, start_time in validated_clips
        ]
        placeholders = ",".join(["(?, ?)" for _ in validated_tuples])
        exclusion_clause = f'AND (fullPath, "start time") NOT IN ({placeholders})'
        exclusion_params = [
            item
            for filename, start_time in validated_tuples
            for item in [filename, start_time]
        ]
    else:
        exclusion_clause = ""
        exclusion_params = []

    query = f"""
    SELECT fullPath, deployment_id, "start time", "scientific name",
           confidence, "max uncertainty", userID
    FROM '{EXPERT_PARQUET_DATASET}'
    WHERE CAST(userID AS VARCHAR) = CAST(? AS VARCHAR)
    {exclusion_clause}
    ORDER BY fullPath, "start time"
    LIMIT 1
    """

    try:
        result = conn.execute(query, [user_id] + exclusion_params).fetchone()

        if result:
            return {
                "filename": result[0],
                "deployment_id": result[1],
                "start_time": result[2],
                "species_array": result[3],
                "confidence_array": result[4],
                "uncertainty_array": result[5],
                "userID": result[6],
                "all_validated": False,
            }
        else:
            query = (
                f"SELECT COUNT(*) FROM '{EXPERT_PARQUET_DATASET}' "
                "WHERE CAST(userID AS VARCHAR) = CAST(? AS VARCHAR)"
            )
            total = conn.execute(query, [user_id]).fetchone()[0]

            if total > 0:
                return {"all_validated": True, "total_clips": total}
            else:
                return None
    except Exception as e:
        st.error(f"Error fetching random clip: {e}")
        return None


@st.cache_data
def get_remaining_pro_clips_count(user_id):
    """Get count of remaining unvalidated clips for user.
    Uses DuckDB COUNT for performance.
    """
    conn = get_duckdb_connection()
    validated_clips = _get_validated_clips_with_session(user_id)

    if validated_clips:
        validated_tuples = [
            (filename, start_time) for filename, start_time in validated_clips
        ]
        placeholders = ",".join(["(?, ?)" for _ in validated_tuples])
        exclusion_clause = f'AND (fullPath, "start time") NOT IN ({placeholders})'
        exclusion_params = [
            item
            for filename, start_time in validated_tuples
            for item in [filename, start_time]
        ]
    else:
        exclusion_clause = ""
        exclusion_params = []

    query = (
        f"SELECT COUNT(*) FROM '{EXPERT_PARQUET_DATASET}' "
        f"WHERE CAST(userID AS VARCHAR) = CAST(? AS VARCHAR) "
        f"{exclusion_clause}"
    )

    try:
        result = conn.execute(query, [user_id] + exclusion_params).fetchone()
        return result[0] if result else 0
    except Exception:
        return 0
